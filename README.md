# whatever
I want a model that compares a word image and a text string, and returns high similarity only if the characters match exactly, in the correct order — no semantic meaning, no “close enough”. Most popular models today (like CLIP, BERT, or GPT) are trained to understand language and meaning, so they think "text" and "paragraph" or "image" and "photo" are similar — but I only care if the strings literally match. That’s why there’s no ready-made solution for this kind of exact visual-text matching. Even if I use a character-level encoder (like one-hot input with a CNN), a naive setup could mistakenly treat "image" and "imgae" as similar because they contain the same letters — just scrambled. To fix that, I must use a model that preserves character order, like a 1D CNN with position-aware filters, or a GRU/LSTM that reads characters in sequence. This lets the model learn that "image" and "imgae" are different — without ever learning what "image" means. The key is to train the model only on exact character sequences using contrastive or triplet loss, and avoid all pretrained embeddings that carry semantic bias.
